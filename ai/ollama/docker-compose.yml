services:
  # LLM and OCR
  # Status check at http://localhost:11434/
  ollama:
    container_name: ollama
    image: ollama/ollama

    # AMD GPU
    privileged: true # needed for GPU
    environment:
      - OLLAMA_VULKAN=1

    # NVIDIA GPU or CPU
    # runtime: nvidia ## NVIDIA GPU required obviously
    # environment:
    #   - NVIDIA_VISIBLE_DEVICES=all

    restart: unless-stopped
    networks:
      - exist
    ports:
      - 11434:11434 # Ollama's default API port

    volumes:
      - ollama_data:/root/.ollama
      - ./entrypoint.sh:/entrypoint.sh:Z # Mount the script
    entrypoint: ["/entrypoint.sh"] # Use the custom script

volumes:
  ollama_data:

networks:
  exist:
    external: true
